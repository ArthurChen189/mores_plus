{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import jax\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import data\n",
    "import modeling_bart\n",
    "import arguments\n",
    "import datasets\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/home/arthur/Workplace/URA/jimmy/mores_plus/training'\n",
    "TRAIN_DATA = '/home/arthur/Workplace/URA/jimmy/mores_plus/training/train_dataset'\n",
    "DEV_DATA = '/home/arthur/Workplace/URA/jimmy/mores_plus/training/dev_dataset'\n",
    "RANK_SCORE_PATH = '/home/arthur/Workplace/URA/jimmy/mores_plus/training'\n",
    "\n",
    "PATH_TO_TSV = '/home/arthur/Workplace/URA/jimmy/mores_plus/training/msmarco-docs.tsv'\n",
    "\n",
    "data_args = arguments.DataArguments(train_dir=TRAIN_DIR,train_path=TRAIN_DATA,dev_path=DEV_DATA,rank_score_path=RANK_SCORE_PATH)\n",
    "reranker_args = arguments.RerankerTrainingArguments(output_dir=os.path.join(TRAIN_DIR,'output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.BartConfig()\n",
    "tokenizer = transformers.BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = modeling_bart.FlaxBartMoresRanker(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.GroupedTrainDataset(args=data_args,path_to_tsv=PATH_TO_TSV,tokenizer=tokenizer,train_args=reranker_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset(data_files='training/msmarco-docs.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/envs/flax/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441704 queries not judged and skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-862e81ca74ac2f89\n",
      "Found cached dataset csv (/home/arthur/.cache/huggingface/datasets/csv/default-862e81ca74ac2f89/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.35it/s]\n",
      "Using custom data configuration default-dab58c8e68de5015\n",
      "Found cached dataset csv (/home/arthur/.cache/huggingface/datasets/csv/default-dab58c8e68de5015/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1184.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import datasets\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "args = {'tokenizer_name':'facebook/bart-base', 'rank_file':'training/run.msmarco-passage.bm25.train.tsv', 'json_dir':'training/json_files',\n",
    "'n_sample':10,'sample_from_top':100,'qrel':'training/document/msmarco-doctrain-qrels.tsv.gz','query_collection': 'training/document/msmarco-doctrain-queries.tsv',\n",
    "'doc_collection':'training/document/msmarco-docs.tsv'}\n",
    "\n",
    "def read_qrel():\n",
    "    import gzip, csv\n",
    "    qrel = {}\n",
    "    with gzip.open(args['qrel'], 'rt', encoding='utf8') as f:\n",
    "        tsvreader = csv.reader(f, delimiter=\" \")\n",
    "        for [topicid, _, docid, rel] in tsvreader:\n",
    "            assert rel == \"1\"\n",
    "            if topicid in qrel:\n",
    "                qrel[topicid].append(docid)\n",
    "            else:\n",
    "                qrel[topicid] = [docid]\n",
    "    return qrel\n",
    "\n",
    "\n",
    "qrel = read_qrel()\n",
    "rankings = defaultdict(list)\n",
    "no_judge = set()\n",
    "with open(args['rank_file']) as f:\n",
    "    for l in f:\n",
    "        qid, pid, rank = l.split()\n",
    "        if qid not in qrel:\n",
    "            no_judge.add(qid)\n",
    "            continue\n",
    "        if pid in qrel[qid]:\n",
    "            continue\n",
    "        # append passage if & only if it is not juddged relevant but ranks high\n",
    "        rankings[qid].append(pid)\n",
    "\n",
    "print(f'{len(no_judge)} queries not judged and skipped', flush=True)\n",
    "\n",
    "columns = ['did', 'url', 'title', 'body']\n",
    "collection = args['doc_collection']\n",
    "collection = datasets.load_dataset(\n",
    "    'csv',\n",
    "    data_files=collection,\n",
    "    column_names=['did', 'url', 'title', 'body'],\n",
    "    delimiter='\\t',\n",
    "    ignore_verifications=True,\n",
    ")['train']\n",
    "qry_collection = args['query_collection']\n",
    "qry_collection = datasets.load_dataset(\n",
    "    'csv',\n",
    "    data_files=qry_collection,\n",
    "    column_names=['qid', 'qry'],\n",
    "    delimiter='\\t',\n",
    "    ignore_verifications=True,\n",
    ")['train']\n",
    "\n",
    "doc_map = {x['did']: idx for idx, x in enumerate(collection)}\n",
    "qry_map = {str(x['qid']): idx for idx, x in enumerate(qry_collection)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['tokenizer_name'], use_fast=True)\n",
    "\n",
    "out_file = args['rank_file']\n",
    "if out_file.endswith('.tsv') or out_file.endswith('.txt'):\n",
    "    out_file = out_file[:-4]\n",
    "out_file = os.path.join(args['json_dir'], os.path.split(out_file)[1])\n",
    "out_file = out_file + '.group.json'\n",
    "\n",
    "queries = list(rankings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/367007 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'D7282917'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb#ch0000012?line=9'>10</a>\u001b[0m neg_encoded \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb#ch0000012?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m neg \u001b[39min\u001b[39;00m negs:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb#ch0000012?line=11'>12</a>\u001b[0m     idx \u001b[39m=\u001b[39m doc_map[\u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mneg]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb#ch0000012?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(idx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Workplace/URA/jimmy/mores_plus/training.ipynb#ch0000012?line=13'>14</a>\u001b[0m     item \u001b[39m=\u001b[39m collection[idx]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'D7282917'"
     ]
    }
   ],
   "source": [
    "with open(out_file, 'w') as f:\n",
    "    for qid in tqdm(queries):\n",
    "        # pick from top of the full initial ranking\n",
    "        negs = rankings[qid][:args['sample_from_top']]\n",
    "        # shuffle if random flag is on\n",
    "        random.shuffle(negs)\n",
    "        # pick n samples\n",
    "        negs = negs[:args['n_sample']]\n",
    "\n",
    "        neg_encoded = []\n",
    "        for neg in negs:\n",
    "            idx = doc_map['D'+neg]\n",
    "            print(idx)\n",
    "            item = collection[idx]\n",
    "            did, url, title, body = (item[k] for k in columns)\n",
    "            url, title, body = map(lambda v: v if v else '', [url, title, body])\n",
    "            encoded_neg = tokenizer.encode(\n",
    "                url + tokenizer.sep_token + title + tokenizer.sep_token + body,\n",
    "                add_special_tokens=False,\n",
    "                max_length=args.truncate,\n",
    "                truncation=True\n",
    "            )\n",
    "            neg_encoded.append({\n",
    "                'passage': encoded_neg,\n",
    "                'pid': neg,\n",
    "            })\n",
    "        pos_encoded = []\n",
    "        for pos in qrel[qid]:\n",
    "            idx = doc_map[pos]\n",
    "            item = collection[idx]\n",
    "            did, url, title, body = (item[k] for k in columns)\n",
    "            url, title, body = map(lambda v: v if v else '', [url, title, body])\n",
    "            encoded_pos = tokenizer.encode(\n",
    "                url + tokenizer.sep_token + title + tokenizer.sep_token + body,\n",
    "                add_special_tokens=False,\n",
    "                max_length=args.truncate,\n",
    "                truncation=True\n",
    "            )\n",
    "            pos_encoded.append({\n",
    "                'passage': encoded_pos,\n",
    "                'pid': pos,\n",
    "            })\n",
    "        q_idx = qry_map[qid]\n",
    "        query_dict = {\n",
    "            'qid': qid,\n",
    "            'query': tokenizer.encode(\n",
    "                qry_collection[q_idx]['qry'],\n",
    "                add_special_tokens=False,\n",
    "                max_length=args.truncate,\n",
    "                truncation=True),\n",
    "        }\n",
    "        item_set = {\n",
    "            'qry': query_dict,\n",
    "            'pos': pos_encoded,\n",
    "            'neg': neg_encoded,\n",
    "        }\n",
    "        f.write(json.dumps(item_set) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3213834"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce192cc7f25a26e7f74bc48c6e381b56c5d8d841b5bd0d44111db8eabaee7fe6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('flax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
